% ビルドメモ
% bibtex では、うまくいかない。
% jbibtex はインストールされていない。
% pbibtex を使うと旨くいく。
% 次の手順でうまくいく。
% $ platex template-j
% $ pbibtex template-j
% $ platex template-j
% $ platex template-j
% $ dvipdfmx template-j

%% 和文論文用のテンプレート
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 1. 和文原稿
 \documentclass[originalpaper]{jsaiart}     % 原著論文 Original Paper
% \documentclass[blindreview]{jsaiart}      % 査読用
%
% \documentclass[shortpaper]{jsaiart}       % 速報論文 Short Paper
% \documentclass[exploratorypaper]{jsaiart} % 萌芽論文 Exploratory Research Paper
% \documentclass[Specialissue]{jsaiart}     % 特集 Special Issue
% \documentclass[specialissue]{jsaiart}     % 小特集 Special Issue
% \documentclass[interimreport]{jsaiart}    % 報告 An Interim Report
% \documentclass[surveypaper]{jsaiart}      % 解説 Survey Paper
% \documentclass[aimap]{jsaiart}            % AIマップ AI map
% \documentclass[specialpaper]{jsaiart}     % 特集論文 Special Paper
% \documentclass[invitedpaper]{jsaiart}     % 招待論文 Invited Paper
%

%% ページ番号の指定，掲載時に学会の方で決定します．
% \setcounter{page}{1}
% \setcounter{volpage}{1}


%%% amsmathパッケージの注意点 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \usepackpage{amsmath}
% 数式番号の参照は \ref ではなく，\eqref を用いること
% documentclass のオプションに fleqnを指定すること
% 例: \documentclass[technicalpaper,fleqn]{jsaiart}

\usepackage[dvipdfmx]{graphicx}
\usepackage{graphics}
\usepackage{fancybox}
\usepackage{comment}
\usepackage{tabularx}
\usepackage{array,booktabs}
\usepackage{diagbox}
\usepackage{multirow}
%\usepackage{setspace}
%\usepackage{longtable} どうやら jsaiart では使えない (T_T)
\newcommand{\bhline}[1]{\noalign{\hrule height #1}}

\Vol{12}
\No{1}
%\jtitle{物体検出のためのニューラルネットワークのサーベイ}
\jtitle{物体検出に用いられるニューラルネットワークモデル}
% \jtitle[柱用和文タイトル]{和文タイトル}
\jsubtitle{最新モデルのサーベイと目的に応じたモデルの選択}
%\etitle{A Survey of Deep Neural Networks for Object Detection}
\etitle{Neural Network Models for Object Detection}
\esubtitle{A Survey of the Latest Models and Optimal Model Selections for Specific Tasks}

\manyauthor % 著者が3名以下の場合はこの行を消すこと

%%% 著者名の注意点 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 所属先が同じ著者が連続する場合，その中の先頭の著者のみ \affiliation
% を用い，残りの所属先には \sameaffiliation を使う
% ただし，所属先が同じでも連続していない場合は \affliation を使う
% 名前が長い場合は \name の代りに \longname を使う

\author{%
 \name{金子}{純也}{Junya Kaneko}
 \affiliation{Morning Project Samurai 株式会社}%
     {Morning Project Samurai Inc.}%
     {junya@mpsamurai.com, http://www.mpsamurai.com}
%\and
% \name{著者2姓}{名}{Auther2 Roman Name}
% \affiliation{日本語所属名2}%
%     {Affiliation2 in English}%
%     {user2@ai-gakkai.or.jp, http://www.ai-gakkai.or.jp/~user2/}
\and
 \name{山田}{貢己}{Miki Yamada}
 \sameaffiliation{m.yamada@mpsamurai.com}
%\and
% \longname{カタガナガキノ}{ナガイナガイナマエ}{VeryLong Roman Name}
% \sameaffiliation{user4@ai-gakkai.or.jp, http://www.ai-gakkai.or.jp/~user4/}
%\and
% \name{著者5姓}{名}{Auther5 Roman Name}
% \affiliation{日本語所属名1}%
%     {Affiliation1 in English}%
%     {user5@ai-gakkai.or.jp, http://www.ai-gakkai.or.jp/~user5/}
}

\begin{keyword}
%キーワードとして，小文字(固有名詞や略語を除く)の英単語を2〜5個指定
    survey, neural network, object detection, instance segmentation, deep learning
\end{keyword}

\begin{summary}
「ショートノート」は 200 ワード，それ以外は200〜500 ワード
以内の英文でsummaryを記す
(ここは，論文執筆後に書く．)
\end{summary}

\begin{document}
\maketitle

\section{まえがき}

%\begin{table*}
%    \caption{物体検出のモデル選択．}
%    \label{tbl-select-h}
%    \begin{center}
%        %\begin{tabularx}{\linewidth}{Xp{1.5cm}Xp{7cm}X} \toprule
%        \begin{tabularx}{\linewidth}{|p{1.3cm}|p{1.0cm}|X|X|X|} \hline
%        %\begin{tabular}{|l|c|c|c|} \hline
%            \multicolumn{2}{|c|}{} & \multicolumn{3}{c|}{画素毎のクラス分類} \\ \cline{3-5}
%            \multicolumn{2}{|c|}{} & \centering{全画素} & \centering{物体領域のみ} & \multicolumn{1}{c|}{しない} \\ \hline
%           \multirow{3}{*}{物体検出} & \multirow{2}{*}{する} & Panoptic segmentation \cite{KHGRD19} & Instance segmentation ( YOLACT++ \cite{BZXL20}，MS R-CNN \cite{HHGHW19} など ) & 物体検出 ( YOLOv4 \cite{BWL20}, EfficientDet \cite{TPL20} など ) \\ \cline{2-5}
%             & しない & Semantic segmentation & \centering{---} & \multicolumn{1}{c|}{---} \\ \hline
%        %\end{tabular}
%        \end{tabularx}
%    \end{center}
%\end{table*}
\begin{table*}
    \caption{物体検出に関連するニューラルネットワークモデル．}
    \label{tbl-select-l}
    \begin{center}
        \setlength{\doublerulesep}{0.5pt}
        \begin{tabularx}{\linewidth}{|p{1.7cm}|p{1.0cm}||X|p{3.3cm}|} \hline
        %\begin{tabularx}{\linewidth}{|p{1.7cm}|p{1.0cm}||X|p{3.3cm}|} \toprule
            \multicolumn{2}{|c||}{} & \multicolumn{2}{c|}{物体検出} \\ \cline{3-4}
            \multicolumn{2}{|l||}{} & \centering{する} & \multicolumn{1}{c|}{しない} \\ \hline\hline
            画素毎の & 全画素 & Panoptic segmentation \cite{KHGRD19} & Semantic segmentation \\ \cline{2-4}
            クラス分類 & 物体領域のみ  & Instance segmentation ( YOLACT++ \cite{BZXL20}，MS R-CNN \cite{HHGHW19} など ) &  \multicolumn{1}{c|}{---} \\ \cline{2-4}
            & しない & 物体検出 ( YOLOv4 \cite{BWL20}, EfficientDet \cite{TPL20} など ) & \multicolumn{1}{c|}{---} \\ 
            \hline
            %\bottomrule
        \end{tabularx}
    \end{center}
\end{table*}

\begin{table*}
    \caption{物体検出のオプション技術．}
    \label{tbl-select-sub}
    \begin{center}
        \setlength{\tabcolsep}{3pt}
        \footnotesize
        %\begin{tabular}{|c|c|c|} \hline
        %\begin{tabular}{|c|p{2cm}|p{2cm}|} \hline
        \begin{tabularx}{\linewidth}{Xp{1.5cm}Xp{7cm}X} \toprule
            \centering{モデル名称} & \centering{文献} & \centering{用途} & \centering{概要} & \multicolumn{1}{c}{特徴} \\ \midrule

            Few-Shot Object Detection with Attention-RPN and Multi-Relation Detector &  & Few-shot 物体検出 & 
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item ★作成予定．
            \end{itemize}
            &
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item ★作成予定．
            \end{itemize}
            \\

            Two-stage Fine-tuning Approach (TFA) & \cite{WHGDY20} & Few-shot 物体検出 & 
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item Few-shot 物体検出のための2ステージfine-tuningを提案．最終層のみfine-tuningする．
                \item Faster R-CNNがそのまま使われている(few-shot学習方法が新しい)．
                \item 特徴抽出器は backbone(ResNet, VGG16等)，RPN，FC sub-networkを含む．
            \end{itemize}
            &
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item メタ学習の従来手法よりも2〜20ポイント優れている．ただし，少数サンプルの分散が大きいと信頼性が低下する．
            \end{itemize}
            \\

            Feature mimic method & \cite{LJY17} & 物体検出の蒸留 & 
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item 物体検出に対して有効な蒸留方法．
                \item 大きなネットワークの特徴マップを教師として学習．大小ネットそれぞれRoIからサンプリングした特徴を用い，小ネットの特徴は変換層を用いて大ネットのサイズにマッピングする．
            \end{itemize}
            & 物体検出に対して，初めて有効な蒸留の方法を示した．\\

            Grad-CAM & \cite{SCDVPB17,SCDVPB20} & 画像中の重要領域の可視化 & 
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item Gradient-weighted Class Activation Mapping (Grad-CAM)を提案．任意の目的概念(イヌ，人，花瓶，…)の流れの最終convolution層での勾配を使い，その概念を予測するため重要な画像中の領域を示す局所化マップを生成．
                \item どのモデルに対しても，アーキテクチャ変更や再学習無しに適用可能．
            \end{itemize}
            &
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item Non-attention ベースのモデルであっても，入力画像における判別領域を可視化できる．
            \end{itemize}
            \\
            \bottomrule
        \end{tabularx}
    \end{center}
\end{table*}%

\begin{table*}
    \caption{物体検出のための主なニューラルネットワークモデル（１）(Segmentationなし)．}
    \label{tbl-cheat1}
    \begin{center}
        \setlength{\tabcolsep}{3pt}
        \footnotesize
        %\begin{tabular}{|c|c|c|} \hline
        %\begin{tabular}{|c|p{2cm}|p{2cm}|} \hline
        \begin{tabularx}{\linewidth}{Xp{1.5cm}Xp{7cm}X} \toprule
            \centering{モデル名称} & \centering{文献} & \centering{用途} & \centering{概要} & \multicolumn{1}{c}{特徴} \\ \midrule

            ResNet & \cite{HZRS16} & DNN & 
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item フィードフォワード型のニューラルネットで最もよく使われる代表的なもの．
                \item  深い階層にもかかわらず高速に学習が可能．
                %\vspace{-0.5\baselineskip}
            \end{itemize} &   \\
            %\\[-8pt] \midrule[0.01pt] \\[-6pt]
            
            Feature Pyramid Network (FPN) & \cite{LDGHHB17} & 物体検出 & 
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item 複数サイズの検出モデルの多くで採用．
                \item コストのわずかな増加でCNNに特徴ピラミッドを導入．
                \item Faster R-CNNと組み合わせて，COCO2016トップの成績を達成． 
            %\end{itemize}\vspace{0.5\baselineskip} 
            \end{itemize}
            & 物体検出の多くのモデルにおいて，その構造の一部として使われている．\\
            %\\[-8pt] \midrule[0.01pt] \\[-6pt]

            Faster R-CNN  & \cite{RHGS15} & 物体検出 & 
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item Two-stage 検出モデルの基準となっている．Fast R-CNNを大きく改良したもの．
                \item 画像ピラミッドの代わりに複数サイズの anchor box を用いて複数サイズの物体を検出．
                \item まずRPNだけをEnd-to-Endで学習．次に固定AnchorとGrandTruthに基づき，物体/背景(2$k$)，ずれ(4$k$)を推論するように全体を学習．
            %\end{itemize}\vspace{0.5\baselineskip} & 高速で検出性能が高い． \\ 
            \end{itemize} & ILSVRC \& COCO 2015 Competitions において(segmentationを含む)5部門で1位．さらに，当時としては処理も高速．\\ 

            RetinaNet & \cite{LGGHD17} & 物体検出 & 
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item YOLACTにおいてベースのモデルとして使われている．
                \item one-stageの検出器の新しい損失関数 Focal loss を提案．Focal Lossを導入したシンプルな検出器をRetinaNet と呼ぶ．
                \item Focal Loss は膨大な数の easy negatives が誤差関数に影響を与えすぎることを防ぐ．
                \item ROIの形にかかわらず各レベル同じ領域の情報から推論する．候補が絞られないからNegative候補の個数は莫大($10^4$〜$10^5$)になる(one-stage detectorの動作の特徴)．
                \item 従来のcross entropyは，誤差関数として考えると $p{>}$0.6 ならば既に学習済みと考えられるが，曲線はほぼ線形で，$p{>}$0.6 の領域でもさらに学習を進めてしまい悪影響を与えていた．
            \end{itemize}
            &
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item one-stage detectorであるにもかかわらず，性能が従来のtwo-stage detectorの最高性能に追いついた．
            \end{itemize}
            \\

            YOLOv3 & \cite{RedFar18} & 物体検出 & 
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item YOLOv2 の改良(one-stage detector)．
                \item 320 × 320 YOLOv3 は 22 ms で走り，SSD と同じ精度 28.2 mAP であり，スピードは3倍速い．
                \item 過去のYOLOは小さな物体が苦手だったが，YOLOv3は大きい物体が苦手．
                \item FPNのように3つの異なるスケールの特徴量を抽出．
                %\vspace{-0.7\baselineskip}
            \end{itemize}
            & \\
            %\bhline{0.1pt} \\

            EfficientDet & \cite{TPL20} & 物体検出 & 
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item -
            \end{itemize}
            &
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item MS COCO Object Detection の AP の最高性能を達成(YOLOv4発表時点)．
            \end{itemize}
            \\

            YOLOv4 & \cite{BWL20} & 物体検出 & 
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item YOLOv3 の改良(one-stage 検出器)．
                \item 以下の新しい特徴を組み合わせて、state-of-the-art の結果を得た： WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, CIoU loss．
                \item 43.5\%AP (65.7\% AP 50 ) for the MS COCO dataset at a real-time speed of ∼65 FPS on Tesla V100.
            \end{itemize}
            &
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item EfficientDet 相当の性能を達成しつつ，速度2倍を達成．
                \item YOLOv3と比べてAPを10ポイント，FPSを12ポイント向上させた．
            \end{itemize}
            \\
            \bottomrule
        \end{tabularx}
    \end{center}
\end{table*}%

\begin{table*}
    \caption{物体検出のための主なニューラルネットワークモデル（２）(Segmentationあり)．}
    \label{tbl-cheat2}
    \begin{center}
        \setlength{\tabcolsep}{3pt}
        \footnotesize
        %\begin{tabular}{|c|c|c|} \hline
        %\begin{tabular}{|c|p{2cm}|p{2cm}|} \hline
        \begin{tabularx}{\linewidth}{Xp{1.5cm}Xp{7cm}X} \toprule
            \centering{モデル名称} & \centering{文献} & \centering{用途} & \centering{概要} & \multicolumn{1}{c}{特徴} \\ \midrule

            Mask R-CNN & \cite{HGDG17} & Instance Segmentation & 
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item Faster R-CNN を拡張し，並列にマスクを予測するブランチを追加．PANetのベースとなるモデル．
                \item BackboneはFPNが使われることがある．
                \item 計算量はFaster R-CNNより少し増える程度．
            \end{itemize}
            &
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item Bbox検出も併用していることで，安定した性能を達成．
            \end{itemize}
            \\

            Path Aggregation Network (PANet) & \cite{LQQSJ18} & Instance Segmentation & 
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item Mask R-CNN + FPN に改良を加えた．
                \item Bottom-up path augmentation により下位層から上位層への情報経路を短くして，元画像の正確な位置情報を特徴量と関係づける．
                \item Adaptive feature pooling が全ての特徴レベルをリンクして直接後続に伝える．
            \end{itemize}
            &
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item  多くの改良点の合わせ技で数ポイントの精度向上を達成．
                \item COCO2017 Instance Segmentationで1位相当の性能を達成．
            \end{itemize}
            \\

            Mask Scoring R-CNN (MS R-CNN) & \cite{HHGHW19} & Instance Segmentation & 
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item 予測マスクの品質を学習する MaskIoU Head を備えた， Mask Scoring R-CNN (two-stage 検出器) を提案．
                \item MaskIoU Head によりマスク品質を回帰推定する．
                \item 推論時に、推定マスク品質を分類スコアに掛け算して補正し，マスク品質が悪いのに分類スコアが大きくなってしまうことを抑制する．
            \end{itemize}
            &
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item Mask R-CNN を抜いて，インスタンスセグメンテーションの最高性能を達成．
            \end{itemize}
            \\

            YOLACT++ & \cite{BZXL20} & Instance Segmentation & 
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item RetinaNet (物体検出) にブランチを幾つか追加してマスク予測機能を持たせたモデル (one-stage 方式)．
                \item 実時間 ($>$30 fps) Instance Segmentationであって，MS COCO に対して最高性能相当を達成．
                \item Fast NMS を提案し，12msほど計算時間を短縮した．
                \item Deformable convolution をbackboneに導入．
                \item Re-Scoring Networkを導入してマスク品質に基づいてマスク予測を再格付けする等の改良．
                \item 34.1 mAP on MS COCO at 33.5fps を達成．
                \item 検出anchorの(スケールとアスペクト比の)選択を工夫．
            \end{itemize}
            &
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item 実時間動作を満たしつつ，Mask R-CNN に近いセグメンテーション性能を達成．
            \end{itemize}
            \\

            Panoptic Segmentation & \cite{KHGRD19} & Panoptic Segmentation & 
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item 画像の全画素に対してクラス分類を行い(semantic segmentation)，且つ，物体に関しては個体を区別して画素単位で識別する(instance segmentation)．
                \item 単に，semantic segmentation と instance segmentation の両方を行えば済むかもしれないが，学習時に両方の教師データが与えられることは片方だけの場合に比べて情報量が多いため，一つのモデルで両方の処理を行わせた場合に推論精度が良くなる可能性がある(★裏を取る必要あり)．
            \end{itemize}
            &
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item -
            \end{itemize}
            \\
            \bottomrule
        \end{tabularx}
    \end{center}
\end{table*}%
\begin{table*}
    \caption{物体検出の性能向上技術．}
    \label{tbl-option}
    \begin{center}
        \setlength{\tabcolsep}{3pt}
        \footnotesize
        %\begin{tabular}{|c|c|c|} \hline
        %\begin{tabular}{|c|p{2cm}|p{2cm}|} \hline
        \begin{tabularx}{\linewidth}{Xp{1.5cm}Xp{7cm}X} \toprule
            \centering{モデル名称} & \centering{文献} & \centering{用途} & \centering{概要} & \multicolumn{1}{c}{特徴} \\ \midrule

            CSPNet & \cite{WLWCHY20} & CNNの軽量化 & 
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item Cross Stage Partial Network (CSPNet) を提案し，従来法を軽くする．
                \item ImageNetデータセットにおいて，同等以上の精度で計算量を20\%削減．MS COCOの物体検出データセットではAP50に関しては最高性能を達成．
                \item CSPNetは簡単に実装できて，ResNet, ResNeXt, DenseNet に基づくアーキテクチャを扱える汎用性を持つ．
            \end{itemize}
            &
            \\

            Convolutional Block Attention Module (CBAM) & \cite{WPLK18} & CNNの性能向上． & 
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item 任意のCNNに適用可能な attention module．
                \item 分類と検出の評価実験において一貫して性能向上を示しており，オーバーヘッドは殆ど無い．
                \item 可視化するときは、Grad-CAM を使用．
            \end{itemize}
            &
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item 全ての場合において，ベースラインよりも性能を向上させた．
            \end{itemize}
            \\

            DropBlock & \cite{GLL18} & 学習性能の向上 & 
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item Convolution層に対して有効な，構造化されたdropout方法．特徴マップの連続した領域をドロップさせる．
                \item ImageNet分類では，ResNet-50 に適用すると1.6ポイント精度が向上．COCO detection では RetinaNet のAPを36.8\%から38.4\%に向上させた．
            \end{itemize}
            &
            \\

            CutMix & \cite{YHCOYC19} & データ拡張 & 
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item 学習画像間で、画像のパッチをカット＆ペーストし，正解ラベルもパッチの面積に比例させてミックスする．
            \end{itemize}
            &
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item ImageNet(Cls), ImageNet(Loc), Pascal VOC Detection に関して，代表的な従来法（Mixup, Cutout）よりも良い性能を示す．
            \end{itemize}
            \\

            SGDR (Cosine annealing scheduler) & \cite{LoshHut17} & 学習性能の向上 & 
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item 周期的にSGDの warm restart をすることを提案．各restartにて，学習率を徐々に小さくするようにスケジューリングする．
                \item YOLOv4で使われている Cosine annealing scheduler はこれのこと．
            \end{itemize}
            &
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item Warm reatart付きのSGDは，従来法に比べて，同じ性能に至るまでのepoch数が1/2から1/4で済む．
            \end{itemize}
            \\

            Distance-IoU (DIoU) 損失／Complete IoU (CIoU)損失
            & \cite{ZWLLYR20} & 学習性能の向上 & 
            \begin{itemize}
                \vspace{-0.7\baselineskip}
                \setlength{\leftskip}{-3mm}
                \item 予測boxと目標boxとの正規化距離を用いた Distance-IoU (DIoU) 損失を提案（従来はLn損失等が使われることも多かった）．
                \item DIoU は IoU損失やGIoU損失より学習の収束が速い．
                \item さらに，追加でアスペクト比を直接損失に組み込んだ，収束が速く性能が良い Complete IoU (CIoU)損失を提案する．
                \item DIoU と CIoU を，YOLOv3, SSD, Faster R-CNN に導入することで，性能を向上させることができる．
                \item DIoU は NMS に簡単に適用することができる．
            \end{itemize}
            &
            \\
            \bottomrule
        \end{tabularx}
    \end{center}
\end{table*}%


%■ 本論文の目的：誰(理工系大学２年生)を対象に、何(物体検出の始め方)を伝えるか
%\subsubsection*{この論文の狙い}
{\bf この論文の狙い:\ }このサーベイは，理工系大学2年生程度の数学の知識を前提に，物体検出をこれから始めるにはどうすればよいかという道すじを伝えることを目的として執筆したものである．物体検出でできることは何?から始まり，物体検出をするために必要なもの(ハード，ソフト，データ，知識，明確な目的)を簡潔に纏めてある．また，読者にとっての理想的なサーベイ(即時性，分かりやすさ，一言で説明，参考文献は充実)というものの一つの解として，随時更新されるGitHub上の(日本語で書かれた)物体検出まとめサイトとライブラリを紹介する．

%■ 最近の世の中の状況のおさらい
{\bf 世の中の状況:\ }近年，「AI(人工知能)」という言葉が国内外に蔓延しており，技術者のみならず一般の人の日常生活にもすっかり浸透した．常に手の届くところにAIがあり，AIに囲まれて生活していると言っても過言ではない．テレビやインターネットの画像は，本物と見間違えるほどの人工画像で溢れ，スマホや机上のスピーカーに話しかけるとあらゆる情報を教えてくれるばかりでなく，電化製品を操作することもできるようになった．高速道路を自動運転する車も増えている．

%■ AI分野(画像認識分野)における物体検出の位置づけ
{\bf AI分野における物体検出:\ }ここ数年で飛躍的に性能を向上させたAI関連技術は，画像認識，物体検出，ロボット制御，音声認識，機械翻訳，ビッグデータ分析などであり，これらの多くの領域で深層ニューラルネットワーク(Deep Neural Network(DNN))が使われている．とりわけ画像認識分野でこのDNNが注目されるようになったのは，2012年に開催された最先端の一般物体認識の性能を競うコンテスト ILSVRC においてDNNを使った手法が他の手法に大差をつけて優勝したことが発端である．「画像中の物は何か?」に答える物体認識をさらに進めて，「画像中のどこに何があるか?」に答えようとするものが本論文のテーマ「物体検出」であり，現在のAIブームを巻き起こした源流がここにあると言ってよい．

%■ 物体検出でできること
{\bf 物体検出でできること:\ }物体検出とは，カメラで撮影された画像データを電子的に処理し，予め登録しておいた物体(例えば，人，犬，猫，自動車，飛行機，...)を見つけ出し，その正確な画像上の位置と物体の種類を予測するものである(「予測(predict)」は，「推定(estimate)」，「推論(inference)」などとも呼ばれ全て同じ意味で使われる)．

現在の標準的方法においては，予め，検出したい対象の学習データ(物体が写っている画像，物体の種類，物体の位置を示す矩形の座標)を大量に用意し，画像を入力すれば種類と位置を出力するように，ニューラルネットワーク等の予測モデルを学習させる．通常，これに数時間から数日要すると言われている．

学習が完了した予測モデルの能力は，タスクの種類によっては人間の能力(予測結果のスコアの平均値)を超えたと言われているものもある(物体認識など)．ただし，物体が写し出された画像の品質(解像度，ノイズ，露出不足/過多)や撮影アングル(遮蔽物，変形，大き(小さ)過ぎる)に問題がある場合は性能が低下することは避けられない．

物体検出と似た技術として，次の3つがある:

\begin{itemize}
    \item セマンティックセグメンテーション(全画素の物体の種類を認識するが，同種の物体同士は区別しない)
    \item インスタンスセグメンテーション(同種の異なる個体を区別して物体検出を行い，且つ，画素単位で個体の識別をする)
    \item パノプティックセグメンテーション(全画素の物体の種類を認識し，同種の異なる個体も区別する)
\end{itemize}
本論文では上記のセグメンテーション技術も含めた広い意味での物体検出について述べる．

%■ 物体検出をするために必要なもの(ハード、ソフト、データ、知識、明確な目的)
{\bf 物体検出をするために必要なもの:}

\begin{description}
    \item[ソフトウェア]　
    \begin{itemize}
        \item PyTorch/TensorFlow 等の深層学習ライブラリとその稼働環境(Linux/Windows/Mac上のPython, jupyter notebook環境など)．
        \item 物体検出を行うソフトウェア(予測モデルの作者，または，物体検出を行おうとする担当者が作ったもの)．
    \end{itemize}
    \item[ハードウェア]　
    \begin{itemize}
        \item 前記ソフトウェアが実行できる環境(PC(GPUがあると良い), 或いは，Google Corabolatory などのサーバ上の実行環境)．
    \end{itemize}
    \item[データ]　
    \begin{itemize}
        \item 学習データ(事前学習用，並びに，fine-tuning用の入力と出力のペア)
        \item 本来処理したいデータ(入力)．
    \end{itemize}
    これらは，使用するソフトウェアで読み取ることのできる状態にしておく(データの前処理)．
    \item[知識] 物体検出のソフトウェアを使うには，入出力データの意味を理解する必要がある．特に，予測モデルの出力データは通常は誤差を含むものとなるため，出力が表す数値が確率値を表すのか，何らかの物理量を表すのか，分類のカテゴリを表すのか，正確に把握する必要がある．また，学習時の損失関数の値から，予測モデルの推定誤差を見積もることができるのだが，それには出力結果を正しく解釈できる統計学の知識が必要となる．
    \item[明確な目的] 何がしたいのかということを明確化しておくことがが，物体検出を行おうとするときに重要となる．物体検出は新しい技術であり，標準的な統計解析の手法よりも手間と計算コストが大きくなりがちである．他の方法では解決できないのか？と問いかけて，本当にこれが必要であることを確認しておくべきである．
\end{description}

%■ 良いサーベイとは(即時性、分かりやすさ、一言で説明＋参考文献)(随時更新する)まとめサイトとライブラリ(GitHub)の紹介。
{\bf 理想的なサーベイとは:\ }最新技術のサーベイ論文は，有用であり様々な分野で昔から(論文雑誌が生まれた頃から)活用されていると思われる．しかしながら，進歩の速い分野においてはサーベイが出た頃には既に内容が古くなってしまっているという問題が往々にして起こる．また，とても良く書かれたサーベイほど内容が濃く多くなり，執筆に時間と労力を要するのはもちろん，それを読み解くのにも時間を要するということもよくある．

我々は，github上に随時更新される形式でサーベイを公開することを試みた．出版されたときには既に古くなっているという懸念を取り払える可能性を期待している．また，この分野に新規参入しようとしている人がなるべく短時間で必要な情報にたどり着き，取り組んでいる問題を解決する最適な方法を見つけたり，或いは，新たな研究に取り組めることを目指し，内容の拡充性や緻密性よりも，なるべく視覚的に解りやすいコンパクトな内容になるよう心掛けた．

\section{物体検出(Object detection)}
\subsection{物体検出器(object detector)の働き}
{\bf 基本動作:\ }物体検出器は，画像(1枚の静止画をファイルにしたもの)を処理し，処理結果(検出個数，検出物体の画像座標，検出物体の種類)を出力する．セグメンテーションの場合は，出力解像度(画像のサイズ)に応じた各画素のクラス分類結果も出力される．画像を読み込ませる際には，幅と高さを含む情報も与える必要がある．検出器によっては画像サイズや画像ファイル形式が指定されているものもあるのでその場合は，予め画像ファイルを変換する前処理が必要となる．

{\bf 出力結果の見かた:\ }検出座標は物体を囲む矩形(bounding box (以下，bbox))の座標を４つの数値で表すことが多い．また，検出の信頼度が0.0〜1.0の実数で出力される場合はそれが推定正答確率を表すように設定されている．

{\bf 学習のしかた:\ }学習データは，推論実行に必要なデータに正解データを加えたものであり，いわゆる教師あり学習を行わせる．ただ，推論時には無い学習に関するパラメタの設定をしなければならない．検出器の構成(中間層の層数，特徴量次元のサイズ，検出器独自のパラメタなど)もこの段階で設定する．通常は，確率的降下法でモデルのパラメタを学習させることが多く，検出器の重みパラメタの初期化方法(平均，分散，値を指定など)，最適化方法(SGD, Adam, 他)，学習率のスケジューリング，学習打ち切り基準，ミニバッチサイズなどを指定する．学習時には交差検証を行わせて未学習データに対する誤差も計算させることができるので，その誤差を控えておくことにより推論時の予測精度を見積もることができる．

{\bf 事前学習(pre-training)と事後学習(post-training, fine-tuning):\ }世界最高性能を出すほどの検出器の学習は，たいてい事前学習と事後学習の2段階で行われる．事前学習は主に公開データベースなどの大量データを用いて検出器の前半部分を学習させて適切な内部表現を獲得するために行われる．検出器によっては事前学習済みの重み係数パラメタが公開されているものもある．事後学習は，最終目的に合致したデータを追加して，場合によっては検出器の最終層を追加して，所望の検出処理を実行できるように最終調整の意味合いで実施するものである．事前/事後学習のやり方は各検出器によって異なるため，説明書きや論文等に記された方法を参考にして実行する必要がある．

%{\bf Two-stage 検出器と one-stage 検出器:\ }
物体検出器は次の2種類に分類できる\cite{JZLYLFQ19}：
\begin{description}
    \item[Two-stage 検出器] 高い位置決め精度と物体認識精度をもつ(Faster R-CNN など)．最初のCNNベースの物体検出器 R-CNN とその改良手法はこの方式である．次の2つのstageで処理される：
    %\begin{description}
    \begin{itemize}
        \item 第1 stage: 物体の bbox の候補を選出する．Faster R-CNN では Region Proposal Network (RPN) と呼ばれている．
        \item 第2 stage: RoI Pooling が各候補boxから特徴量を切り出し，後続の分類器と bbox 回帰器がそれを処理する．
    %\end{description}
    \end{itemize}
    \item[One-stage 検出器] 推論速度が高速である(YOLO, SSD など)．Region proposal をせずに直接，入力画像から bbox を検出して出力する．近年では，改良により検出精度も向上している．
\end{description}

\subsection{Two-stage検出器}
\subsubsection{Faster R-CNN}
代表的なtwo-stage検出器であり，物体検出に対して当時の最高性能を達成しながら，処理速度の点でも大きく改善したモデルである\cite{RHGS15}．
PASCAL VOC 2007 に対して mAP=69.9\% の精度を 5fps で達成した．
\begin{figure}[tb]
    \begin{center}
        \includegraphics[width=8cm,clip]{fig/archi_FasterRCNN.eps}
    \end{center}
    %\capwidth=90mm %
    \caption{ Faster R-CNN の構造．}
    \label{fig:archi_FasterRCNN}
\end{figure}

{\bf Faster R-CNN の構造:\ } \ref{fig:archi_FasterRCNN}に示すように，backbone，Region Proposal Network (RPN)，RoI Pooling，特徴抽出ネットワーク，分類器，回帰器で構成される．

Backbone は CNN で構成され，$W{\times}H{\times}128$のサイズの特徴量を出力する．提案当時はVGG-16 \cite{SimZis15}などが用いられた．

RPN は backbone が出力した特徴量をさらに CNN で処理して物体の種類に依存せずに物体を検出してその候補(proposalと呼ぶ)の RoI (bbox座標)と物体らしさ(objectness)を $n$ 個ずつ出力する．RPN内部では特徴量をCNNで変換し，特徴量の各空間座標点毎に $k$ 個の「bbox(4個) と物体らしさ(2個)の数値」を同時に出力している(計$WH(4{+}2)k$個)．これらの bbox は non-maximum suppression (NMS) によって，物体らしさが閾値を超えているものだけを残し，また，1個の物体を複数回検出したものも1個だけに集約することにより，RPNが出力するproposal を $n$ 個以下に抑える．ここで，各座標点に割り当てられた $k$ 個の初期 bbox を anchor box と呼んでいる．

RoI Pooling は，backbone からの特徴量と RPN からの $n$個の proposals を受け取り，bbox内部の特徴量を切り出して物体の大きさにかかわらずに空間サイズを 7{$\times$}7 の固定サイズに変換する．

固定サイズの特徴量は，共通の特徴抽出を行う全結合(Full Connect)層(以下，FC層)により $n{\times}1024$のデータに変換され，分類器と回帰器に送られて別々のFC層で処理されて，クラス分類確率と bbox 調整量がそれぞれ出力される．(★bbox調整量が全クラス分出力されるのかどうか確認要！)

{\bf Faster R-CNN の学習:\ } 
\begin{enumerate}
    \item CNNによる特徴量生成は，画像認識などの学習で構築された重みを初期値として fine tuning を行う．
    \item RPNだけをEnd-to-Endで学習．anchorと教師データに基づき，物体か背景か(物体らしさ，2k個)と，anchorからのずれ(4k個)を学習．
    \item RPN固定で全体を学習する．RPNのRoI出力を全体の出力及びRoI poolingへ送り，特徴抽出器で分類と回帰の共通部分のFC計算を行う．分類器はクロスエントロピー，softmax などで計算し，回帰器が行う線形回帰はL1ノルムで学習し，bboxの位置の補正を行う．
\end{enumerate}

\subsubsection{Two-Stage Fine-Tuning Approach (TFA)}
Few-shot 物体検出のためのシンプルな two-stage の fine-tuning法である\cite{WHGDY20}．
元来，深層学習は膨大な個数の学習データが必要であり，実用上の観点では学習データを集めることが困難な場合があった．それゆえ，少ないサンプルから珍しいオブジェクトを見つけることを目的とする few-shot 物体検出の開発が進められている。
few-shot 学習の方法としてはメタ学習が有望と見られてきたが，このTFAはそれまで注目されてこなかった fine-tuning に焦点をあてることで大きな性能向上をもたらした．
最終層のみ fine-tuning することが few-shot 物体検出にとって重要であることを発見し，それを用いた Two-stage Fine-tuning Approach (TFA)を提案した．
メタ学習の従来手法よりも 2〜20ポイント高い精度を出している．ただし，少数サンプルの分散が大きいと信頼性が低下すると言われている．
ベースとなる検出モデルは，Faster R-CNN がそのまま使われている．本手法はネットワーク構造には殆ど依存せず，few-shot 学習方法に特徴がある．

\begin{figure}[tb]
    \begin{center}
        \includegraphics[width=8cm,clip]{fig/archi_TFA.eps}
    \end{center}
    %\capwidth=90mm %
    \caption{ TFA の Few-shot fine-tuning (2nd stage)．}
    \label{fig:archi_TFA}
\end{figure}
{\bf Two-stage fine-tuning approach (TFA) の学習:\ } 
\begin{enumerate}
    \item Base model training (1st stage) \\
    特徴抽出器$\mathcal{F}$と box 予測器(分類器, 回帰器)をbaseクラスの学習セット $C_{\rm b}$ で学習する．そのときの損失関数はFaster R-CNN \cite{RHGS15}と同じ $$L=L_{\rm rpn}+L_{\rm cls}+L_{\rm loc}$$を用いる．$L_{\rm rpn}$ はRPNの出力に適用され，前景を背景から区別し，anchorを学習データに近づける．$L_{\rm cls}$は box 分類器 C に対するクロスエントロピー損失，$L_{\rm loc}$は回帰器 R に対する平滑化L1損失である．
    \item Few-shot fine-tuning (2nd stage) \\
    バランスのとれた1クラスあたり $K$ shotsの学習セットを用意する．これらは base クラス $C_{\rm b}$ と novel クラス $C_{\rm n}$ を含む．Box 予測器の重みは乱数初期化する．特徴抽出器$\mathcal{F}$を固定してBox分類器と回帰器(検出モデルの最終層)をfine-tuningする．1st stage と同じ前記の損失関数 $L$ を用い，学習率は 1st stage から 20だけ減じる．また，2nd stage ではcosine 類似度を用いて分類器の損失を正規化する工夫を取り入れている．
\end{enumerate}

\subsubsection{Few-Shot Object Detection}

\subsection{One-stage検出器}
\subsubsection{YOLOv4}
\begin{figure}[tb]
    \begin{center}
        \includegraphics[width=8cm,clip]{fig/archi_YOLOv4.eps}
        %\includegraphics[width=8cm,clip]{fig/testYOLO.eps}
    \end{center}
    %\capwidth=90mm %
    \caption{ YOLOv4 の構造．}
    \label{fig:archi_YOLOv4}
\end{figure}
代表的なone-stage検出器であり，高速，高性能で軽量であることが特長である\cite{BWL20}．YOLOv3までの特徴を継承しつつ，採用可能なbackbone，neck, head から最適な組み合わせを選択し，また，適用可能な各種技術を取り込んで構成された(\ref{fig:archi_YOLOv4})．

{\bf YOLOv4の構造:\ }入力画像は，backbone(画像の特徴を抽出する処理(縦横サイズを小さくしながら特徴量を含むチャネルを増やしていくこと))，neck(FPNやPANのように、特徴量ピラミッドを昇り降り横断する処理)，head(「backbone + neck」が出力する特徴量から、クラス信頼度とbounding boxオフセットを推論するネットワーク)の順に処理される．

backbone である CSPDarknet53 は，YOLOv3 で使われた Darknet53 に Cross Stage Partial Network (CSP)を導入したものである．Darknet53 は，residual結合を持ちConvolution 2層で構成されるブロックが多数積み重なった構造であり，名前の53はConvolutionが53個あることから来ているとしている(ただし，53番目の層は全結合層である)．CSP network は予測性能を向上させる工夫であり，注目するブロックへの特徴量入力を２つに分割し，一方はそのブロックで処理し，もう一方は処理をスキップしてそのまま送り，これら２つを連結(concatenation)して出力することを行う．ただし，YOLOv4においては，この「分割」処理の代わりに，分岐させた直後にstride=2のConvolutionで処理して両方のチャネルサイズを1/2にしている．

CSPDarknet53のTopの出力はSPPモジュール(kernel size={1, 5, 9, 13}, stride=1として，4つ並列にmax poolingを行い，これらをconcatするもの)に送られる．比較的大きなk$\times$k max-poolingが効果的にbackbone特徴量の受容野を増加させてから，neckのTopに渡される．

neck である Modified PAN は，Path Aggregation Network (PAN)(3つのピラミッドを行き来して高解像度情報を特徴マップに効果的に伝えるモデル)におけるbottom-up path の加算計算をconcatenationに変更したモデルが用いられている．

ここではさらに，Modified Spatial Attention Module (SAM)(Convolutionの出力を2つに分岐し，一方にConvolution + sigmoid処理を行い，元信号に掛け算する)の演算を行ったものがPANの3個の出力としてheadに渡される．

3つのheadは，それぞれ独立にconvolution計算を2回行い，最終的なクラス信頼度とbounding boxオフセットを出力する．

{\bf YOLOv4の学習:\ }一般的な確率的降下法を用いた学習に加えて，いくつか効果的な学習の工夫を導入している: 

\begin{description}
    \item[CutMix] 学習画像の一部を切り取ったパッチを別の学習画像の一部に貼り付けて，正解ラベルもパッチの面積に比例させてミックスしたものを生成してそれで学習する．
    \item[Mosaicデータ拡張] CutMixを，4つの画像を用いるように拡張したもの．
    \item[DropBlock] 特徴マップに対して，無作為に選出した矩形範囲(block)にマスクを掛けたもので学習する．
    \item[Class label smoothing] クロスエントロピー損失で学習する際に，logitが発散しないように正解ラベルを$q'(k|x)=(1-\epsilon)\delta_{k,y}+\epsilon/K$とした損失を用いて学習する．
    \item[Complete IoU (CIoU) 損失] 二つのbounding boxが離れていても，近づきすぎていても適切な損失関数になるように，IoU損失を改良した損失関数．
    \item[CmBN] batch normalizationを(ミニバッチ複数回につき1回の重み更新を行う場合に)ミニバッチをまたいで「平均，分散」を蓄積して，重みの更新のタイミングで，biasとscaleを更新するようにしたものである．
    \item[Self-adversarial-training (SAT)] まず，物体を写した元画像を改変し，物体が存在しない画像であると騙す画像を生成する．次に，この改変された画像中の物体を検出するように通常の学習を行う．
    \item[Cosine annealing scheduler] コサイン関数の半周期の形状を利用して，学習率を少しずつ減少させる．これを周期を増やしながら複数回繰り返す．
\end{description}
\subsubsection{EfficientDet}

\section{インスタンスセグメンテーション(Instance segmentation)}
\subsection{Mask Scoring R-CNN (MS R-CNN)}
%\begin{figure*}[b] %これにすると独立したページに１段組で出力された.
\begin{figure}[tb]
    \begin{center}
        \includegraphics[width=8cm,clip]{fig/archi_ms_rcnn.eps}
    \end{center}
    %\capwidth=90mm %
    \caption{ Mask Scoring R-CNN の構造．}
    \label{fig:archi_ms_rcnn}
\end{figure}
マスク品質(インスタンスマスクと正解マスクとのIoUとして定量化されるもの)を分類スコアと明示的に関連付けたモデルである\cite{HHGHW19}．MS R-CNN は，予測マスクの品質を学習するためのブロック(MaskIoU Head)を，Mask R-CNN\cite{HGDG17} に導入したモデルになっている(\ref{fig:archi_ms_rcnn})．MaskIoU Headはインスタンスの特徴量と対応する予測マスクを一緒に取り込み，それを元にMask IoU を回帰推定する．そして，推論時に予測MaskIoUを分類スコアに掛け算して補正する．
\subsubsection{MS R-CNN の学習}
学習サンプルとして RPN proposals を使う．
proposal box と正解box との IoU が 0.5 以上の学習サンプルが必要となる．これは Mask R-CNN の Mask head の学習サンプルの場合と同じである．
各学習サンプルに対する回帰目標を生成するために，まず目標クラスの予測マスクを取得し，予測マスクを閾値=0.5で2値化する．そして，2値化マスクと正解との MaskIoU を使う．
MaskIoUを回帰するのには L2 損失を使い，損失重みは1にする．
ネットワーク全体は end-to-end で学習する．
\subsubsection{MS R-CNN の推論処理}
MaskIoU Head は分類スコア(R-CNN head の出力)の調整に使う．推論の手順は次のようになる:
\begin{enumerate}
    \item R-CNN head が $N$個のbounding boxを出力する．
    \item $N$個のbounding boxのうち，SoftNMS\cite{BSCD17}で上位$k$個のボックスを選択する．
    \item 上位$k$個のボックスを Mask Head に入力し，$k$個のマルチクラスマスクを生成する(ここまでは標準的 Mask R-CNN の手順)．
    \item これら$k$個のマスクを目標として MaskIoU Head に入力し，予測 MaskIoU を出力する．
    \item 予測 MaskIoU を，分類スコアに掛け算し，上位$k$個の修正された分類スコアを得る．
\end{enumerate}

\subsection{YOLACT++}
%\begin{figure*}[b] %これにすると独立したページに１段組で出力された.
\begin{figure}[tb]
    \begin{center}
        \includegraphics[width=8cm,clip]{fig/archi_YOLACT++.eps}
    \end{center}
    %\capwidth=90mm %
    \caption{ YOLACT++ の構造．}
    \label{fig:archi_yolactpp}
\end{figure}  
実時間(${>}$30fps)で動作するインスタンスセグメンテーションのモデルであり，MS COCO に対して当時の最高性能に匹敵する性能(34.1 mAP at 33.5fps)を達成した\cite{BZXL20}．fully-convolutionモデルであり，deformable convolution をYOLACT\cite{BZXL19}のbackboneに導入する等の改良をしている。物体検出モデルのRetinaNet\cite{LGGHD17} をもとに，インスタンスセグメンテーション向けに改良したものである(\ref{fig:archi_yolactpp})．
\subsubsection{YOLACT++ の学習}
easy negative が多くて学習が困難になる問題は，OHEM法\footnote{入力画像に対する全RoIをミニバッチと考えて、損失の値でソートして識別が難しい negative を選択して学習させる方法．}を用いて negative:positive=3:1 にして学習することで対応する．
Class 信頼度は分類損失(クロスエントロピー)，bbox はL1損失，mask (「mask係数×Prototype」で得られるもの)は pixel-wise binary cross entropy でそれぞれ学習する．Re-Scoring Net は Mask IoU(係数)を回帰する学習を行う．

Semantic Segmentation Loss は，学習時のみ接続されるネットワークの学習であり，この学習の実施によりmAPが0.4ポイント向上する．P3特徴量出力に 1×1 convolution 1層で処理してcチャネルの出力をさせて最後にシグモイド関数をかける．これが正解 mask になるように学習する．

%これにより，各画素にc個の[0,1]出力が割り当てられる．
% (おそらく，pixel-wise binary cross entropy 損失と予想)
\subsubsection{YOLACT++ の推論処理}
Prediction Head が各 anchor の Class 信頼度，bbox，k 個の mask 係数を出力し，
Protonet が k 個の Prototype (mask) を出力する．
そして，Predition Head 出力を NMS 処理して選ばれた結果に対して，mask 係数と Prototype を積和した結果(全画面のmask)を，予測 bbox の外側を0で埋めたものを2値化して最終的な予測 mask 出力を得る．

並行して，2値化する前の mask を Re-Scoring Net に入力して，mask IoU 出力を得る．分類スコアは，この mask IoU を掛けて補正される．
\section{パノプティックセグメンテーション(Panoptic segmentation)}
\cite{KHGRD19}．
\section{むすび}

\begin{acknowledgment}
謝辞について
\end{acknowledgment}a

%\bibliography{btxsample}
\bibliography{mps}
\bibliographystyle{jsai}

\appendix

\section{付録のタイトル1}
付録の本文1
%\section{付録のタイトル2}
%付録の本文2

% 著者の姓と名の間は半角スペースで区切る
% 略歴は200字以内
\begin{biography}
    %\profile*{m}{著者姓 名}{前掲\kern-.5zw (Vol.X，No.Y，p.Z)\kern-.5zw 参照．}
\profile{m}{金子 純也}{著者1の略歴}
\profile{m}{山田 貢己}{1989年東京大学大学院物理学専攻修了．理学博士．同年株式会社東芝入社．ニューラルネットワークの研究開発，セキュリティ技術，画像認識技術，テレビの高画質化技術，車載画像認識プロセッサ等の開発業務に従事．2020年ジャパニアス株式会社に入社．現在，Morning Project Samurai 株式会社においてAI開発業務に従事．}
\end{biography}

\end{document}
